{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b863a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T04:13:05.516202Z",
     "start_time": "2021-11-05T04:13:03.970508Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov  2 15:02:57 2021\n",
    "\n",
    "@author: leyuan\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LassoLarsIC, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix  #, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "# from torchsummary import summary\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ================= data cleaning ==============================================\n",
    "dir_name = './real_data'\n",
    "paths = os.listdir('./real_data')\n",
    "automobile_p, student_p, spam_p = [os.path.join(dir_name, path) for path in paths]\n",
    "\n",
    "# automobile = pd.read_csv(automobile_p)\n",
    "# student = pd.read_csv(student_p)\n",
    "spam = pd.read_csv(spam_p)\n",
    "\n",
    "\n",
    "X = spam.iloc[:, :-2].to_numpy()\n",
    "Y = spam.ham.to_numpy() * 1\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================================\n",
    "def get_data(x, y, batch_size=32):\n",
    "#     x = StandardScaler(with_mean=True, with_std=True).fit_transform(x)\n",
    "    sample_size = x.shape[0]\n",
    "    idx = np.random.choice(range(sample_size), batch_size, replace=False)\n",
    "    return x[idx, :], y[idx, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        '''\n",
    "        obs_dim: dim_x or (dim_x + dim_y)\n",
    "        action_dim: dim_x\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=obs_dim, out_features=256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float)\n",
    "        logits = F.relu(self.fc1(obs))\n",
    "        logits = self.fc2(logits)\n",
    "        \n",
    "        m = Bernoulli(logits=logits)\n",
    "        \n",
    "        actions = m.sample()\n",
    "        log_probs = m.log_prob(actions)\n",
    "        entropy = m.entropy()\n",
    "        \n",
    "        return actions, log_probs, entropy\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(X_train, Y_train, X_test, Y_test, actions, num_iter=500, lr=1e-3, batch_size='auto', dictionary=dict()):\n",
    "    reward_list = []\n",
    "    for action in actions.detach().numpy():\n",
    "        \n",
    "        idx = np.where(action == 1)[0]\n",
    "        \n",
    "        if tuple(idx) in dictionary:\n",
    "            reward_list.append(dictionary[tuple(idx)])\n",
    "        else:\n",
    "            X_select = X_train[:, idx]        \n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(128,), random_state=1, learning_rate='adaptive', batch_size=batch_size,\n",
    "                                      learning_rate_init=lr, max_iter=num_iter, tol=1e-3)\n",
    "            classifier.fit(X_select, Y_train)\n",
    "            # X_select = X_test[:, idx] \n",
    "            # probs = classifier.predict_proba(X_select)\n",
    "            # eps = np.where(probs < 1e-4, 1e-4, 0)\n",
    "            # log_probs = np.log(probs + eps)\n",
    "            # log_likelihood = (log_probs[:, 1] * Y_test + log_probs[:, 0] * (1 - Y_test)).mean()\n",
    "            # predict_proba = classifier.predict_proba(X_select)\n",
    "            # loss = log_loss(Y_test, predict_proba)\n",
    "            # dictionary[tuple(idx)] = loss\n",
    "            # reward_list.append(loss)\n",
    "            \n",
    "            # classifier = RandomForestClassifier(max_depth=5)\n",
    "            # classifier = SVC(gamma='auto')\n",
    "            # classifier = LogisticRegression()\n",
    "            # classifier.fit(X_select, Y_train)\n",
    "            \n",
    "            X_select = X_test[:, idx] \n",
    "            score = classifier.score(X_select, Y_test)\n",
    "            dictionary[tuple(idx)] = 1 - score\n",
    "            reward_list.append(1 - score)\n",
    "        \n",
    "    return np.array(reward_list)\n",
    "\n",
    "# ==============================================================================================\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=3)\n",
    "\n",
    "def metrics(idx, x_train, x_test, y_train, y_test):\n",
    "    x_train_sel = x_train[:, idx]\n",
    "    x_test_sel = x_test[:, idx]\n",
    "    \n",
    "    \n",
    "    svc_sel = SVC(gamma='auto')\n",
    "    svc_sel.fit(x_train_sel, y_train)\n",
    "    \n",
    "    return svc_sel.score(x_test_sel, y_test)\n",
    "\n",
    "\n",
    "m = 2576\n",
    "n = 57\n",
    "\n",
    "def run(seed):\n",
    "    start = time.time()\n",
    "    print(f'random seed: {seed} is running')\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "    \n",
    "    actor = Actor(obs_dim=n, action_dim=n)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "        \n",
    "    action_select = []\n",
    "    dictionary = dict()\n",
    "    r_list = []\n",
    "    \n",
    "    r_baseline = torch.tensor(0)\n",
    "    \n",
    "    x_tt, x_val, y_tt, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "    \n",
    "    for step in range(200):\n",
    "        # print('step: ', step)\n",
    "        \n",
    "        X_train, Y_train = get_data(x_tt, y_tt, batch_size=64)\n",
    "            \n",
    "        actions, log_probs, entropy = actor(X_train)\n",
    "        action_select.append(actions.detach().numpy().mean(axis=0))\n",
    "        \n",
    "        # r_baseline = critic(X_train)\n",
    "        # r_baseline = r_baseline.squeeze()\n",
    "        \n",
    "        \n",
    "        rewards = compute_reward(x_tt, y_tt, x_val, y_val, actions, num_iter=800, lr=1e-2, batch_size=64, dictionary=dictionary)\n",
    "        r_list.append(rewards.mean())\n",
    "        # print(f'average reward: {rewards.mean()}')\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        \n",
    "        r_baseline = 0.95 * r_baseline + 0.05 * rewards.mean()\n",
    "        \n",
    "        # update actor\n",
    "        actor_loss =  ((rewards - r_baseline) * log_probs.sum(dim=-1)).mean()\n",
    "        # actor_loss =  (rewards * log_probs.sum(dim=-1)).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        # print(f'actor loss: {actor_loss.item()}')\n",
    "        \n",
    "        # actor_loss =  (rewards * log_probs.sum(dim=-1)).mean()\n",
    "        # actor_optimizer.zero_grad()\n",
    "        # actor_loss.backward()\n",
    "        # actor_optimizer.step()\n",
    "        # print(f'actor loss: {actor_loss.item()}\\n')\n",
    "        \n",
    "        # update critic\n",
    "        # critic_loss = F.mse_loss(r_baseline, rewards)\n",
    "        # critic_optimizer.zero_grad()\n",
    "        # critic_loss.backward()\n",
    "        # critic_optimizer.step()\n",
    "        # print(f'critic loss: {critic_loss.item()}\\n')\n",
    "        \n",
    "        if step > 6:\n",
    "            if (abs(r_list[-1] - r_list[-2]) < 1e-3) & (abs(r_list[-2] - r_list[-3]) < 1e-3) & (abs(r_list[-3] - r_list[-4]) < 1e-3) & (abs(r_list[-4] - r_list[-5]) < 1e-3):\n",
    "#             print(f'converge at step {step}')\n",
    "                break\n",
    "    \n",
    "    action_select = np.array(action_select)\n",
    "            \n",
    "    \n",
    "    tmp = sorted(dictionary.items(), key=lambda x: x[1])\n",
    "    s = set(range(n))\n",
    "    for item in tmp[:5]:\n",
    "        s = s & set(item[0])\n",
    "    # print(s)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        actions, log_probs, _ = actor(x_train)\n",
    "    \n",
    "    \n",
    "    acp1 = np.where(np.array(action_select[-10:]).mean(axis=0) > 0.9)[0]\n",
    "    acp2 = (torch.where(actions.mean(dim=0) > 0.9)[0]).numpy()\n",
    "    acp3 = np.array(list(s))\n",
    "    \n",
    "    logistic_cv = LogisticRegressionCV(cv=5, fit_intercept=False, penalty='l1', max_iter=1e6, solver='saga')\n",
    "    logistic_cv.fit(x_train, y_train)\n",
    "    lr1 = np.where(logistic_cv.coef_.ravel() != 0)[0]\n",
    "    \n",
    "#     regr = LogisticRegression(penalty='l2', fit_intercept=False, max_iter=1e6)\n",
    "#     regr.fit(x_train, y_train)\n",
    "#     lr_sfm = SelectFromModel(regr, prefit=True)\n",
    "#     lr2 = np.where(lr_sfm.get_support())[0]\n",
    "    \n",
    "    \n",
    "    regr = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "    regr.fit(x_train, y_train)\n",
    "    rf_sfm = SelectFromModel(regr, prefit=True)\n",
    "    rf = np.where(rf_sfm.get_support())[0]\n",
    "    \n",
    "    \n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    dat = np.zeros((2, 6))\n",
    "    dat[0, 5] = 57; dat[1, 5] = svc.score(x_test, y_test)\n",
    "    dat[0, 0] = len(acp1); dat[0, 1] = len(acp2); dat[0, 2] = len(acp3); dat[0, 3] = len(lr1); dat[0, 4] = len(rf)\n",
    "    dat[1, 0] = metrics(acp1, x_train, x_test, y_train, y_test)\n",
    "    dat[1, 1] = metrics(acp2, x_train, x_test, y_train, y_test)\n",
    "    dat[1, 2] = metrics(acp3, x_train, x_test, y_train, y_test)\n",
    "    dat[1, 3] = metrics(lr1, x_train, x_test, y_train, y_test)\n",
    "    dat[1, 4] = metrics(rf, x_train, x_test, y_train, y_test)    \n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'rd: {seed} take {datetime.timedelta(seconds = end - start)}')\n",
    "    \n",
    "    return dat\n",
    "\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# results_add = []\n",
    "# for sd in tqdm(range(6, 8)): \n",
    "#     results_add.append(run(sd))\n",
    "# end = time.time()\n",
    "# print(datetime.timedelta(seconds = end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f364be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:17:26.431110Z",
     "start_time": "2021-11-05T07:17:26.341456Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov  2 15:02:57 2021\n",
    "\n",
    "@author: leyuan\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import sklearn\n",
    "from sklearn.linear_model import LinearRegression, LassoLarsIC, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix  #, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Bernoulli\n",
    "# from torchsummary import summary\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# ================= data cleaning ==============================================\n",
    "dir_name = './real_data'\n",
    "paths = os.listdir('./real_data')\n",
    "automobile_p, student_p, spam_p = [os.path.join(dir_name, path) for path in paths]\n",
    "\n",
    "# automobile = pd.read_csv(automobile_p)\n",
    "# student = pd.read_csv(student_p)\n",
    "spam = pd.read_csv(spam_p)\n",
    "\n",
    "\n",
    "X = spam.iloc[:, :-2].to_numpy()\n",
    "Y = spam.ham.to_numpy() * 1\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================================\n",
    "def get_data(x, y, batch_size=32):\n",
    "#     x = StandardScaler(with_mean=True, with_std=True).fit_transform(x)\n",
    "    sample_size = x.shape[0]\n",
    "    idx = np.random.choice(range(sample_size), batch_size, replace=False)\n",
    "    return x[idx, :], y[idx, np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        '''\n",
    "        obs_dim: dim_x or (dim_x + dim_y)\n",
    "        action_dim: dim_x\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=obs_dim, out_features=256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float)\n",
    "        logits = F.relu(self.fc1(obs))\n",
    "        logits = self.fc2(logits)\n",
    "        \n",
    "        m = Bernoulli(logits=logits)\n",
    "        \n",
    "        actions = m.sample()\n",
    "        log_probs = m.log_prob(actions)\n",
    "        entropy = m.entropy()\n",
    "        \n",
    "        return actions, log_probs, entropy\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(X_train, Y_train, X_test, Y_test, actions, num_iter=500, lr=1e-3, batch_size='auto', dictionary=dict()):\n",
    "    reward_list = []\n",
    "    for action in actions.detach().numpy():\n",
    "        \n",
    "        idx = np.where(action == 1)[0]\n",
    "        \n",
    "        if tuple(idx) in dictionary:\n",
    "            reward_list.append(dictionary[tuple(idx)])\n",
    "        else:\n",
    "            X_select = X_train[:, idx]        \n",
    "            classifier = MLPClassifier(hidden_layer_sizes=(128,), random_state=1, learning_rate='adaptive', batch_size=batch_size,\n",
    "                                      learning_rate_init=lr, max_iter=num_iter, tol=1e-3)\n",
    "            classifier.fit(X_select, Y_train)\n",
    "            # X_select = X_test[:, idx] \n",
    "            # probs = classifier.predict_proba(X_select)\n",
    "            # eps = np.where(probs < 1e-4, 1e-4, 0)\n",
    "            # log_probs = np.log(probs + eps)\n",
    "            # log_likelihood = (log_probs[:, 1] * Y_test + log_probs[:, 0] * (1 - Y_test)).mean()\n",
    "            # predict_proba = classifier.predict_proba(X_select)\n",
    "            # loss = log_loss(Y_test, predict_proba)\n",
    "            # dictionary[tuple(idx)] = loss\n",
    "            # reward_list.append(loss)\n",
    "            \n",
    "            # classifier = RandomForestClassifier(max_depth=5)\n",
    "            # classifier = SVC(gamma='auto')\n",
    "            # classifier = LogisticRegression()\n",
    "            # classifier.fit(X_select, Y_train)\n",
    "            \n",
    "            X_select = X_test[:, idx] \n",
    "            score = classifier.score(X_select, Y_test)\n",
    "            dictionary[tuple(idx)] = 1 - score\n",
    "            reward_list.append(1 - score)\n",
    "        \n",
    "    return np.array(reward_list)\n",
    "\n",
    "# ==============================================================================================\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=3)\n",
    "\n",
    "def metrics(idx, x_train, x_test, y_train, y_test):\n",
    "    x_train_sel = x_train[:, idx]\n",
    "    x_test_sel = x_test[:, idx]\n",
    "    \n",
    "    \n",
    "    svc_sel = SVC(gamma='auto')\n",
    "    svc_sel.fit(x_train_sel, y_train)\n",
    "    \n",
    "    return svc_sel.score(x_test_sel, y_test)\n",
    "\n",
    "\n",
    "m = 2576\n",
    "n = 57\n",
    "\n",
    "def run(seed):\n",
    "    start = time.time()\n",
    "    print(f'random seed: {seed} is running')\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "    \n",
    "#     actor = Actor(obs_dim=n, action_dim=n)\n",
    "#     actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "        \n",
    "#     action_select = []\n",
    "#     dictionary = dict()\n",
    "#     r_list = []\n",
    "    \n",
    "#     r_baseline = torch.tensor(0)\n",
    "    \n",
    "#     x_tt, x_val, y_tt, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "    \n",
    "#     for step in range(200):\n",
    "#         # print('step: ', step)\n",
    "        \n",
    "#         X_train, Y_train = get_data(x_tt, y_tt, batch_size=64)\n",
    "            \n",
    "#         actions, log_probs, entropy = actor(X_train)\n",
    "#         action_select.append(actions.detach().numpy().mean(axis=0))\n",
    "        \n",
    "#         # r_baseline = critic(X_train)\n",
    "#         # r_baseline = r_baseline.squeeze()\n",
    "        \n",
    "        \n",
    "#         rewards = compute_reward(x_tt, y_tt, x_val, y_val, actions, num_iter=800, lr=1e-2, batch_size=64, dictionary=dictionary)\n",
    "#         r_list.append(rewards.mean())\n",
    "#         # print(f'average reward: {rewards.mean()}')\n",
    "#         rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        \n",
    "#         r_baseline = 0.95 * r_baseline + 0.05 * rewards.mean()\n",
    "        \n",
    "#         # update actor\n",
    "#         actor_loss =  ((rewards - r_baseline) * log_probs.sum(dim=-1)).mean()\n",
    "#         # actor_loss =  (rewards * log_probs.sum(dim=-1)).mean()\n",
    "#         actor_optimizer.zero_grad()\n",
    "#         actor_loss.backward()\n",
    "#         actor_optimizer.step()\n",
    "#         # print(f'actor loss: {actor_loss.item()}')\n",
    "        \n",
    "#         # actor_loss =  (rewards * log_probs.sum(dim=-1)).mean()\n",
    "#         # actor_optimizer.zero_grad()\n",
    "#         # actor_loss.backward()\n",
    "#         # actor_optimizer.step()\n",
    "#         # print(f'actor loss: {actor_loss.item()}\\n')\n",
    "        \n",
    "#         # update critic\n",
    "#         # critic_loss = F.mse_loss(r_baseline, rewards)\n",
    "#         # critic_optimizer.zero_grad()\n",
    "#         # critic_loss.backward()\n",
    "#         # critic_optimizer.step()\n",
    "#         # print(f'critic loss: {critic_loss.item()}\\n')\n",
    "        \n",
    "#         if step > 6:\n",
    "#             if (abs(r_list[-1] - r_list[-2]) < 1e-3) & (abs(r_list[-2] - r_list[-3]) < 1e-3) & (abs(r_list[-3] - r_list[-4]) < 1e-3) & (abs(r_list[-4] - r_list[-5]) < 1e-3):\n",
    "# #             print(f'converge at step {step}')\n",
    "#                 break\n",
    "    \n",
    "#     action_select = np.array(action_select)\n",
    "            \n",
    "    \n",
    "#     tmp = sorted(dictionary.items(), key=lambda x: x[1])\n",
    "#     s = set(range(n))\n",
    "#     for item in tmp[:5]:\n",
    "#         s = s & set(item[0])\n",
    "#     # print(s)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         actions, log_probs, _ = actor(x_train)\n",
    "    \n",
    "    \n",
    "#     acp1 = np.where(np.array(action_select[-10:]).mean(axis=0) > 0.9)[0]\n",
    "#     acp2 = (torch.where(actions.mean(dim=0) > 0.9)[0]).numpy()\n",
    "#     acp3 = np.array(list(s))\n",
    "    \n",
    "    logistic_cv = LogisticRegressionCV(cv=5, fit_intercept=False, penalty='l1', max_iter=1e6, solver='saga')\n",
    "    logistic_cv.fit(x_train, y_train)\n",
    "    lr1 = np.where(logistic_cv.coef_.ravel() != 0)[0]\n",
    "    \n",
    "#     regr = LogisticRegression(penalty='l2', fit_intercept=False, max_iter=1e6)\n",
    "#     regr.fit(x_train, y_train)\n",
    "#     lr_sfm = SelectFromModel(regr, prefit=True)\n",
    "#     lr2 = np.where(lr_sfm.get_support())[0]\n",
    "    \n",
    "    \n",
    "    regr = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "    regr.fit(x_train, y_train)\n",
    "    rf_sfm = SelectFromModel(regr, prefit=True)\n",
    "    rf = np.where(rf_sfm.get_support())[0]\n",
    "    \n",
    "    \n",
    "    svc = SVC(gamma='auto')\n",
    "    svc.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    dat = np.zeros((2, 3))\n",
    "    dat[0, 2] = 57; dat[1, 2] = svc.score(x_test, y_test)\n",
    "#     dat[0, 0] = len(acp1); dat[0, 1] = len(acp2); dat[0, 2] = len(acp3); dat[0, 3] = len(lr1); dat[0, 4] = len(rf)\n",
    "#     dat[1, 0] = metrics(acp1, x_train, x_test, y_train, y_test)\n",
    "#     dat[1, 1] = metrics(acp2, x_train, x_test, y_train, y_test)\n",
    "#     dat[1, 2] = metrics(acp3, x_train, x_test, y_train, y_test)\n",
    "#     dat[1, 3] = metrics(lr1, x_train, x_test, y_train, y_test)\n",
    "#     dat[1, 4] = metrics(rf, x_train, x_test, y_train, y_test)    \n",
    "    \n",
    "    dat[0, 0] = len(lr1); dat[0, 1] = len(rf)\n",
    "    dat[1, 0] = metrics(lr1, x_train, x_test, y_train, y_test)\n",
    "    dat[1, 1] = metrics(rf, x_train, x_test, y_train, y_test) \n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'rd: {seed} take {datetime.timedelta(seconds = end - start)}')\n",
    "    \n",
    "    return dat\n",
    "\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# results_add = []\n",
    "# for sd in tqdm(range(6, 8)): \n",
    "#     results_add.append(run(sd))\n",
    "# end = time.time()\n",
    "# print(datetime.timedelta(seconds = end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8796e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:19:37.861033Z",
     "start_time": "2021-11-05T07:17:27.637806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed: 6 is runningrandom seed: 7 is runningrandom seed: 11 is running\n",
      "\n",
      "\n",
      "random seed: 2 is running\n",
      "random seed: 1 is running\n",
      "rd: 6 take 0:02:02.768925\n",
      "rd: 7 take 0:02:07.496912\n",
      "rd: 1 take 0:02:07.690892\n",
      "rd: 2 take 0:02:07.826681\n",
      "rd: 11 take 0:02:10.163627\n",
      "0:02:10.213969\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   # 不加这个multiprocess会报错...\n",
    "    # results = []\n",
    "    # for sd in tqdm(range(20)):\n",
    "    #     results.append(run(sd))\n",
    "\n",
    "    # print(\"CPU的核数为：{}\".format(mp.cpu_count()))\n",
    "    start = time.time()\n",
    "    pool = mp.Pool(5)\n",
    "    dats = pool.map(run, [1, 2, 6, 7, 11])\n",
    "    pool.close() \n",
    "    end = time.time()\n",
    "    print(datetime.timedelta(seconds = end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363a95fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:37:31.213411Z",
     "start_time": "2021-11-05T07:37:31.205747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[57.        , 13.        , 57.        ],\n",
       "        [ 0.80072464,  0.76721014,  0.80072464]],\n",
       "\n",
       "       [[57.        , 16.        , 57.        ],\n",
       "        [ 0.81793478,  0.79800725,  0.81793478]],\n",
       "\n",
       "       [[56.        , 15.        , 57.        ],\n",
       "        [ 0.79438406,  0.78351449,  0.79438406]],\n",
       "\n",
       "       [[57.        , 15.        , 57.        ],\n",
       "        [ 0.80434783,  0.78713768,  0.80434783]],\n",
       "\n",
       "       [[57.        , 15.        , 57.        ],\n",
       "        [ 0.83514493,  0.81612319,  0.83514493]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats = np.array([dat for dat in dats])\n",
    "dats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f2a1c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:37:41.794826Z",
     "start_time": "2021-11-05T07:37:41.788274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.8       , 14.8       , 57.        ],\n",
       "       [ 0.81050725,  0.79039855,  0.81050725]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f739069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:36:23.168948Z",
     "start_time": "2021-11-05T07:36:23.155556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.        , 13.        , 57.        ],\n",
       "       [ 0.80072464,  0.76721014,  0.80072464]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9e55e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:36:26.188889Z",
     "start_time": "2021-11-05T07:36:26.182655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.        , 16.        , 57.        ],\n",
       "       [ 0.81793478,  0.79800725,  0.81793478]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e390091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:36:27.373662Z",
     "start_time": "2021-11-05T07:36:27.367463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.        , 15.        , 57.        ],\n",
       "       [ 0.79438406,  0.78351449,  0.79438406]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536e4782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:36:28.139996Z",
     "start_time": "2021-11-05T07:36:28.133946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.        , 15.        , 57.        ],\n",
       "       [ 0.80434783,  0.78713768,  0.80434783]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6068dc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T07:36:29.027815Z",
     "start_time": "2021-11-05T07:36:29.021719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.        , 15.        , 57.        ],\n",
       "       [ 0.83514493,  0.81612319,  0.83514493]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dats[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6.8",
   "language": "python",
   "name": "py3.6.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
